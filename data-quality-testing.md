# Automated Data Quality Testing
Data quality is essential aspect to any data-related foundation of a business or project. As the saying goes,
"garbage in, garbage out." This fundamental aspect is crucial to any requisite layers on top of
data ingestion, including dashboarding, reporting, UI, analytics, and modeling.

Data quality can be monitored and used to triggered alerts that can help engineers prioritize and
identify root causes for the data issues.

Data quality validation should be automated as much as possible.

Testing applied at every level of the analytics pipelines can help to prevent costly mistakes.
It is crucial in maintaining trust in the systems and platforms that are built off the data.

In engineering, DevOps is the automation of the release process of development code to production,
greatly increasing the reproducibility and quality of code releases. Similar to DevOps, [DataOps](https://en.wikipedia.org/wiki/DataOps) is an automated, process-oriented methodology used to improve the quality of data analytics and reduce the time needed to move ideas from development to production.

Unlike DevOps, DataOps is in its infancy. DataOps was originally introduced in 2014 by Lenny Liebmann in a blog post for InformationWeek. In contrast, DevOps has been around since the early 1990's. As a result, while testing automation is a major foundation of DevOps, [DataOps practitioners](https://s3.amazonaws.com/eckerson/content_assets/assets/000/000/195/original/DataOPS.pdf?1534882627) do not have the breadth of tools available to test data in analytic pipelines.

# Testing business logic
